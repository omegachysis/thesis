{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import PIL.Image, PIL.ImageDraw\n",
    "import base64\n",
    "import zipfile\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pylab as pl\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from IPython.display import Image, HTML, clear_output\n",
    "import tqdm\n",
    "\n",
    "def np2pil(a):\n",
    "    if a.dtype in [np.float32, np.float64]:\n",
    "        a = np.uint8(np.clip(a, 0, 1)*255)\n",
    "    return PIL.Image.fromarray(a)\n",
    "\n",
    "def imwrite(f, a):\n",
    "    a = np.asarray(a)\n",
    "    np2pil(a).save(f, \"png\", quality=95)\n",
    "    \n",
    "def imencode(a):\n",
    "    a = np.asarray(a)\n",
    "    f = io.BytesIO()\n",
    "    imwrite(f, a)\n",
    "    return f.getvalue()\n",
    "\n",
    "def imshow(arr):\n",
    "    display(Image(data=imencode(arr)))\n",
    "    \n",
    "def load_image(path):\n",
    "    img = PIL.Image.open(path)\n",
    "    img = np.float32(img) / 255.0\n",
    "    # Premultiplied alpha:\n",
    "    img[..., :3] *= img[..., 3:]\n",
    "    return img\n",
    "\n",
    "def zoom(img, scale=4):\n",
    "    img = np.repeat(img, scale, 0)\n",
    "    img = np.repeat(img, scale, 1)\n",
    "    return img\n",
    "\n",
    "def to_alpha(x):\n",
    "    return tf.clip_by_value(x[..., 3:4], 0.0, 1.0)\n",
    "\n",
    "def to_rgb(x):\n",
    "    rgb, a = x[..., :3], to_alpha(x)\n",
    "    return 1.0 - a + rgb\n",
    "\n",
    "def to_rgba(x):\n",
    "    return x[..., :4]\n",
    "\n",
    "def tile2d(a, w=None):\n",
    "    a = np.asarray(a)\n",
    "    if w is None:\n",
    "        w = int(np.ceil(np.sqrt(len(a))))\n",
    "    th, tw = a.shape[1:3]\n",
    "    pad = (w-len(a))%w\n",
    "    a = np.pad(a, [(0, pad)]+[(0, 0)]*(a.ndim-1), 'constant')\n",
    "    h = len(a)//w\n",
    "    a = a.reshape([h, w]+list(a.shape[1:]))\n",
    "    a = np.rollaxis(a, 2, 1).reshape([th*h, tw*w]+list(a.shape[4:]))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentParams:\n",
    "\n",
    "    channel_count = 32\n",
    "    \n",
    "    # number of neurons in middle layer:\n",
    "    filter_size = 32\n",
    "    \n",
    "    pool_size = 1\n",
    "    learning_rate = 3.0e-3\n",
    "    \n",
    "    image_data = \"checkerboard8.png\"\n",
    "    \n",
    "    target_loss = -3.0\n",
    "    lifetime = 30\n",
    "    \n",
    "    #num_steps = 1000\n",
    "    #fire_rate = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cellular Automata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              multiple                  3104      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            multiple                  1056      \n",
      "=================================================================\n",
      "Total params: 4,160\n",
      "Trainable params: 4,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class CellularAutomata(tf.keras.Model):\n",
    "    channel_n = ExperimentParams.channel_count\n",
    "    #fire_rate = ExperimentParams.fire_rate\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dmodel = tf.keras.Sequential([\n",
    "            Conv2D(filters = ExperimentParams.filter_size, kernel_size = 1, activation = tf.nn.relu),\n",
    "            Conv2D(filters = self.channel_n, kernel_size = 1,\n",
    "                   activation = None, kernel_initializer = tf.zeros_initializer)\n",
    "        ])\n",
    "\n",
    "        # This call just triggers the model to actually initialize:\n",
    "        self(tf.zeros([1, 3, 3, self.channel_n]))\n",
    "        \n",
    "    #def get_living_mask(self, x):\n",
    "    #    alpha = x[:, :, :, 3:4]\n",
    "    #    return tf.nn.max_pool2d(alpha, 3, [1, 1, 1, 1], \"SAME\") > 0.1\n",
    "\n",
    "    @tf.function\n",
    "    def perceive(self, x):\n",
    "        identity = np.float32([0, 1, 0])\n",
    "        identity = np.outer(identity, identity)\n",
    "\n",
    "        # Sobel filter\n",
    "        dx = np.outer(np.float32([1, 2, 1]), np.float32([-1, 0, 1])) / 8.0\n",
    "        dy = dx.T\n",
    "\n",
    "        kernel = tf.stack(\n",
    "            values = [identity, dx - dy, dx + dy],\n",
    "            axis = -1)[:, :, None, :] # TODO: figure out what this does\n",
    "\n",
    "        kernel = tf.repeat(\n",
    "            input = kernel, \n",
    "            repeats = self.channel_n, \n",
    "            axis = 2)\n",
    "        \n",
    "        return tf.nn.depthwise_conv2d(\n",
    "            input = x, \n",
    "            filter = kernel, \n",
    "            strides = [1, 1, 1, 1],\n",
    "            padding = \"SAME\")\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        #pre_life_mask = self.get_living_mask(x)\n",
    "        \n",
    "        s = self.perceive(x)\n",
    "        x += self.dmodel(s)\n",
    "        \n",
    "        #post_life_mask = self.get_living_mask(x)\n",
    "        #life_mask = pre_life_mask & post_life_mask\n",
    "        return x # * tf.cast(life_mask, tf.float32)\n",
    "    \n",
    "CellularAutomata().dmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__module__': '__main__', 'channel_count': 32, 'filter_size': 32, 'pool_size': 1, 'learning_rate': 0.003, 'image_data': 'checkerboard8.png', 'target_loss': -3.0, 'lifetime': 30, '__dict__': <attribute '__dict__' of 'ExperimentParams' objects>, '__weakref__': <attribute '__weakref__' of 'ExperimentParams' objects>, '__doc__': None}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAYAAACM/rhtAAAAZklEQVR4nO3VIQ7AMAzAwGT///P2gTCDGdiwCjhVrbJz9x5n+8fccwypCkjTA3dEH+Ka099gQJoeeD3eGdHH0d9gQJoe2CahBaTpgW0SWkCaHtgmoQWk6YFtElpAmh7YJqEFpOmBHxq5IEU44lkgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " step: 99, log10(loss): -3.041\n",
      " Time (s): 4.016\n",
      "\n",
      " Best loss: -3.041\n",
      "\n",
      " Num steps: 99\n"
     ]
    }
   ],
   "source": [
    "class SamplePool:\n",
    "    pool_size = ExperimentParams.pool_size\n",
    "    \n",
    "    def __init__(self, *, _parent=None, _parent_idx=None, **slots):\n",
    "        self._parent = _parent\n",
    "        self._parent_idx = _parent_idx\n",
    "        self._slot_names = slots.keys()\n",
    "        self._size = None\n",
    "        for k, v in slots.items():\n",
    "            if self._size is None:\n",
    "                self._size = len(v)\n",
    "            assert self._size == len(v)\n",
    "            setattr(self, k, np.asarray(v))\n",
    "            \n",
    "    def sample(self, n):\n",
    "        idx = np.random.choice(self._size, n, False)\n",
    "        batch = {k: getattr(self, k)[idx] for k in self._slot_names}\n",
    "        batch = SamplePool(**batch, _parent=self, _parent_idx = idx)\n",
    "        return batch\n",
    "    \n",
    "    def commit(self):\n",
    "        for k in self._slot_names:\n",
    "            getattr(self._parent, k)[self._parent_idx] = getattr(self, k)\n",
    "            \n",
    "class TrainingConfig:\n",
    "    def __init__(self):\n",
    "        self.target = load_image(ExperimentParams.image_data)\n",
    "        h, w = self.target.shape[:2]\n",
    "        self.seed = np.zeros([h, w, CellularAutomata.channel_n], np.float32)\n",
    "        \n",
    "        # Placing a seed in the center:\n",
    "        #self.seed[h // 2, w // 2, 3:] = 1.0\n",
    "        self.ca = CellularAutomata()\n",
    "        \n",
    "        # TODO: Verify the random number generator used in the TF optimizer\n",
    "        self.loss_hist = []\n",
    "        self.learning_rate = ExperimentParams.learning_rate\n",
    "        self.lr_sched = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "            boundaries = [2000], \n",
    "            values = [self.learning_rate, self.learning_rate * 0.1])\n",
    "        self.trainer = tf.keras.optimizers.Adam(self.lr_sched)\n",
    "        \n",
    "        self.target_loss = ExperimentParams.target_loss\n",
    "        self.initial_loss = self.get_loss(self.seed).numpy()\n",
    "        self.pool = SamplePool(x = np.repeat(self.seed[None, ...], SamplePool.pool_size, 0))\n",
    "        \n",
    "    def get_loss(self, img):\n",
    "        return tf.reduce_mean(tf.square(to_rgba(img) - self.target), [-2, -3, -1])\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, x):\n",
    "        iter_n = ExperimentParams.lifetime\n",
    "        with tf.GradientTape() as g:\n",
    "            for i in tf.range(iter_n):\n",
    "                x = self.ca(x)\n",
    "            loss = tf.reduce_mean(self.get_loss(x))\n",
    "            \n",
    "        grads = g.gradient(loss, self.ca.weights)\n",
    "        grads = [g / (tf.norm(g) + 1.0e-8) for g in grads]\n",
    "        self.trainer.apply_gradients(zip(grads, self.ca.weights))\n",
    "        return x, loss\n",
    "        \n",
    "    def show_status(self, initial):\n",
    "        #seed_image = np.hstack(to_rgb(initial).numpy())\n",
    "        \n",
    "        # Run the CA for its lifetime with the current weights.\n",
    "        xs = []\n",
    "        x = initial\n",
    "        for i in range(ExperimentParams.lifetime):\n",
    "            x = self.ca(x)\n",
    "            xs.append(x)\n",
    "        \n",
    "        #result_image = np.hstack([to_rgb(x).numpy() for x in xs])\n",
    "        #result_image = np.hstack(to_rgb(xs).numpy())\n",
    "        vis = []\n",
    "        for i in range(len(xs) // 5):\n",
    "            vis.append(np.hstack(\n",
    "                [np.hstack(to_rgb(x).numpy()) for x in xs[5*i : 5*i + 5]]))\n",
    "            \n",
    "        result_image = np.vstack(vis)\n",
    "            \n",
    "        #    [\n",
    "        #    np.hstack(to_rgb(xs[0]).numpy()),\n",
    "        #    np.hstack(to_rgb(xs[1]).numpy())\n",
    "        #])\n",
    "        \n",
    "        #vis = np.vstack([seed_image, result_image])\n",
    "        imshow(zoom(result_image, 3))\n",
    "        print(\"\\n step: %d, log10(loss): %.3f\" % (\n",
    "            len(self.loss_hist), np.log10(self.loss_hist[-1])), end='')\n",
    "    \n",
    "    def run(self, num_steps, batch_size = 1):\n",
    "        initial = result = loss = None\n",
    "        assert num_steps > 0\n",
    "        for i in range(num_steps):\n",
    "            initial = np.repeat(self.seed[None, ...], batch_size, 0)\n",
    "            x, loss = self.train_step(initial)\n",
    "            self.loss_hist.append(loss.numpy())\n",
    "            if np.log10(loss) <= self.target_loss:\n",
    "                break\n",
    "        self.show_status(initial)\n",
    "\n",
    "print(vars(ExperimentParams))\n",
    "training = TrainingConfig()\n",
    "imshow(zoom(training.target, 5))\n",
    "\n",
    "def run():\n",
    "    while not training.loss_hist or min(np.log10(training.loss_hist)) > ExperimentParams.target_loss:\n",
    "        training.run(num_steps = 100)\n",
    "import timeit\n",
    "print(\"\\n Time (s): %.3f\" % timeit.timeit(run, number=1))\n",
    "print(\"\\n Best loss: %.3f\" % min(np.log10(training.loss_hist)))\n",
    "print(\"\\n Num steps: %d\" % len(training.loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: testing for target loss rather than number of steps\n",
    "\n",
    "# Tests on checkerboard 8x8:\n",
    "# (channel_count, filter_size) => (# steps, time)\n",
    "data1 = {\n",
    "    (4, 32): (1130, 22.3),\n",
    "    (128, 32): (112, 5.868),\n",
    "    (256, 32): (88, 7.464),\n",
    "    \n",
    "    (4, 4): None, # timed-out at 15,000 steps\n",
    "    (4, 8): None, # timed-out at 10,000 steps\n",
    "    (4, 16): (3085, 56.941),\n",
    "    \n",
    "    (6, 6): (6458, 120.829),\n",
    "    (8, 8): (2025, 38.033),\n",
    "    (8, 16): (584, 12.230),\n",
    "    \n",
    "    (8, 32): (489, 10.062),\n",
    "    (8, 64): (233, 6.113),\n",
    "    (8, 128): (100, 4.295),\n",
    "    \n",
    "    (32, 32): (237, 7.401),\n",
    "    (64, 64): (83, 3.878),\n",
    "    (128, 128): (95, 7.739),\n",
    "}\n",
    "\n",
    "# Tests on square 8x8:\n",
    "# (channel_count, filter_size) => (# steps, time)\n",
    "data1 = {\n",
    "    (4, 4): None, # couldn't learn at all,\n",
    "    (64, 64): None, # couldn't learn at all\n",
    "}\n",
    "\n",
    "# Tests on guided square 8x8:\n",
    "# (channel_count, filter_size) => (# steps, time)\n",
    "data1 = {\n",
    "    (4, 4): None, #timed-out at 8,300 steps\n",
    "    (32, 32): (74, 3.052),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: testing on various images and trying to improve time to train\n",
    "\n",
    "# Hypothesis: increasing fire rate will improve loss and not affect time\n",
    "data3 = {\n",
    "    0.1: (-0.417, 90),\n",
    "    1.0: (-2.415, 87),\n",
    "}\n",
    "# Removing the update mask calculations entirely reduced time to 84 seconds\n",
    "\n",
    "# Hypothesis: increasing learning rate will first improve loss and then worsen it and not affect time\n",
    "# pool_size => (min_loss, time)\n",
    "data2 = {\n",
    "    0.1e-3: (-0.344, 51),\n",
    "    2.0e-3: (-1.737, 51),\n",
    "    3.0e-3: (-1.742, 52),\n",
    "    5.0e-3: (-1.751, 52),\n",
    "    10.0e-3: (-1.751, 52),\n",
    "    20.0e-3: (-1.639, 53),\n",
    "}\n",
    "\n",
    "# Hypothesis: increasing pool size will improve loss but increase time\n",
    "# pool_size => (min_loss, time)\n",
    "data2 = {\n",
    "    8: (-1.737, 52),\n",
    "    32: (-1.724, 53),\n",
    "    128: (-1.733, 52),\n",
    "    1024: (-1.736, 51),\n",
    "}\n",
    "\n",
    "# Hypothesis: increasing lifetime will improve loss but increase time\n",
    "# lifetime => (min_loss, time)\n",
    "data1 = {\n",
    "    10: (-0.457, 26),\n",
    "    30: (-1.731, 51),\n",
    "    50: (-1.757, 86),\n",
    "    100: (-1.761, 204),\n",
    "    200: (-1.765, 362),\n",
    "}\n",
    "\n",
    "# 1 -> 2 : 10 times\n",
    "# 1 -> 1.1 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: testing on Lenna 32x32 and examining channel counts\n",
    "\n",
    "# Hypothesis: increasing channel count will reduce best loss\n",
    "\n",
    "# Filter size => (Best loss, Time)\n",
    "filter_sizes = {\n",
    "    '4': (-1.674, 134),\n",
    "    '64': (-1.754, 225),\n",
    "}\n",
    "\n",
    "# Channel count => Best Loss\n",
    "channel_counts = {\n",
    "    '4': -1.688,\n",
    "    '5': -1.735,\n",
    "    '6': -1.705,\n",
    "    '8': -1.733,\n",
    "    '10': -1.727,\n",
    "    '12': -1.751,\n",
    "    '14': -1.732,\n",
    "    '16': -1.719,\n",
    "    '32': -1.742,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meeting Notes:\n",
    "\n",
    "- Seed the random numbers the same way, should behave exactly the same\n",
    "    - Look into that kind of thing in Tensorflow\n",
    "- Record the seed when an experiment starts\n",
    "- The parameters are not separable\n",
    "    - Separable parameters are independent, when you fix one it doesn't affect the other\n",
    "    - Try each parameter change with a change in each other parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bit7f79f381b2994605bf95c96e7d268b80"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
