{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import PIL.Image, PIL.ImageDraw\n",
    "import base64\n",
    "import zipfile\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pylab as pl\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from IPython.display import Image, HTML, clear_output\n",
    "import tqdm\n",
    "\n",
    "def np2pil(a):\n",
    "    if a.dtype in [np.float32, np.float64]:\n",
    "        a = np.uint8(np.clip(a, 0, 1)*255)\n",
    "    return PIL.Image.fromarray(a)\n",
    "\n",
    "def imwrite(f, a):\n",
    "    a = np.asarray(a)\n",
    "    np2pil(a).save(f, \"png\", quality=95)\n",
    "    \n",
    "def imencode(a):\n",
    "    a = np.asarray(a)\n",
    "    f = io.BytesIO()\n",
    "    imwrite(f, a)\n",
    "    return f.getvalue()\n",
    "\n",
    "def imshow(arr):\n",
    "    display(Image(data=imencode(arr)))\n",
    "    \n",
    "def load_image(path):\n",
    "    img = PIL.Image.open(path)\n",
    "    img = np.float32(img) / 255.0\n",
    "    return img\n",
    "\n",
    "def zoom(img, scale=4):\n",
    "    img = np.repeat(img, scale, 0)\n",
    "    img = np.repeat(img, scale, 1)\n",
    "    return img\n",
    "\n",
    "def to_rgb(x):\n",
    "    rgb = x[..., :3]\n",
    "    return rgb\n",
    "\n",
    "def tile2d(a, w=None):\n",
    "    a = np.asarray(a)\n",
    "    if w is None:\n",
    "        w = int(np.ceil(np.sqrt(len(a))))\n",
    "    th, tw = a.shape[1:3]\n",
    "    pad = (w-len(a))%w\n",
    "    a = np.pad(a, [(0, pad)]+[(0, 0)]*(a.ndim-1), 'constant')\n",
    "    h = len(a)//w\n",
    "    a = a.reshape([h, w]+list(a.shape[1:]))\n",
    "    a = np.rollaxis(a, 2, 1).reshape([th*h, tw*w]+list(a.shape[4:]))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentParams:\n",
    "\n",
    "    # ----------------------------\n",
    "    # Parameters to tune:\n",
    "    # ----------------------------\n",
    "    channel_count = 32\n",
    "    filter_size = 32 # number of neurons in middle layer\n",
    "    pool_size = 1\n",
    "    learning_rate = 3.0e-3\n",
    "    lifetime = 50\n",
    "\n",
    "    # ----------------------------\n",
    "    # Challenge parameters:\n",
    "    # ----------------------------\n",
    "    image_data = \"checkerboard8.png\"\n",
    "    target_loss = -2.2\n",
    "    noise = 0.1\n",
    "    conserve_mass = False\n",
    "    clamp_values = True\n",
    "    \n",
    "    #seed_mutation_odds = 100000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cellular Automata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellularAutomata(tf.keras.Model):\n",
    "    channel_n = ExperimentParams.channel_count\n",
    "    noise = ExperimentParams.noise\n",
    "    #fire_rate = ExperimentParams.fire_rate\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dmodel = tf.keras.Sequential([\n",
    "            Conv2D(filters = ExperimentParams.filter_size, kernel_size = 1, activation = tf.nn.relu),\n",
    "            Conv2D(filters = self.channel_n, kernel_size = 1,\n",
    "                   activation = None, kernel_initializer = tf.zeros_initializer)\n",
    "        ])\n",
    "\n",
    "        # This call just triggers the model to actually initialize:\n",
    "        self(tf.zeros([1, 3, 3, self.channel_n]))\n",
    "\n",
    "    @tf.function\n",
    "    def perceive(self, x):\n",
    "        identity = np.float32([0, 1, 0])\n",
    "        identity = np.outer(identity, identity)\n",
    "\n",
    "        # Sobel filter\n",
    "        dx = np.outer(np.float32([1, 2, 1]), np.float32([-1, 0, 1])) / 8.0\n",
    "        dy = dx.T\n",
    "\n",
    "        kernel = tf.stack(\n",
    "            values = [identity, dx - dy, dx + dy],\n",
    "            axis = -1)[:, :, None, :] # TODO: figure out what this does\n",
    "\n",
    "        kernel = tf.repeat(\n",
    "            input = kernel, \n",
    "            repeats = self.channel_n, \n",
    "            axis = 2)\n",
    "        \n",
    "        return tf.nn.depthwise_conv2d(\n",
    "            input = x, \n",
    "            filter = kernel, \n",
    "            strides = [1, 1, 1, 1],\n",
    "            padding = \"SAME\")\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        s = self.perceive(x)\n",
    "        dx = self.dmodel(s)\n",
    "        \n",
    "        # Add mass conservation to the model by subtracting the average of the dx values.\n",
    "        if ExperimentParams.conserve_mass:\n",
    "            dx -= tf.math.reduce_mean(dx)\n",
    "            \n",
    "        x += dx\n",
    "        \n",
    "        # Add random noise.\n",
    "        x += (tf.cast(tf.random.uniform(tf.shape(x[:, :, :, :])), tf.float32) - 0.5) * 2.0 * self.noise\n",
    "        \n",
    "        # Keep random noise or changes in dx from causing out-of-range values.\n",
    "        if ExperimentParams.clamp_values:\n",
    "            x = tf.clip_by_value(x, 0.0, 1.0)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "CellularAutomata().dmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplePool:\n",
    "    pool_size = ExperimentParams.pool_size\n",
    "    \n",
    "    def __init__(self, *, _parent=None, _parent_idx=None, **slots):\n",
    "        self._parent = _parent\n",
    "        self._parent_idx = _parent_idx\n",
    "        self._slot_names = slots.keys()\n",
    "        self._size = None\n",
    "        for k, v in slots.items():\n",
    "            if self._size is None:\n",
    "                self._size = len(v)\n",
    "            assert self._size == len(v)\n",
    "            setattr(self, k, np.asarray(v))\n",
    "            \n",
    "    def sample(self, n):\n",
    "        idx = np.random.choice(self._size, n, False)\n",
    "        batch = {k: getattr(self, k)[idx] for k in self._slot_names}\n",
    "        batch = SamplePool(**batch, _parent=self, _parent_idx = idx)\n",
    "        return batch\n",
    "    \n",
    "    def commit(self):\n",
    "        for k in self._slot_names:\n",
    "            getattr(self._parent, k)[self._parent_idx] = getattr(self, k)\n",
    "            \n",
    "class TrainingConfig:\n",
    "    def __init__(self):\n",
    "        self.target = load_image(ExperimentParams.image_data)\n",
    "        self.seed = self.generate_seed_state()\n",
    "        \n",
    "        # Placing a seed in the center:\n",
    "        #self.seed[h // 2, w // 2, 3:] = 1.0\n",
    "        self.ca = CellularAutomata()\n",
    "\n",
    "        self.loss_hist = []\n",
    "        self.learning_rate = ExperimentParams.learning_rate\n",
    "        self.lr_sched = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "            boundaries = [2000], \n",
    "            values = [self.learning_rate, self.learning_rate * 0.1])\n",
    "        self.trainer = tf.keras.optimizers.Adam(self.lr_sched)\n",
    "        \n",
    "        self.target_loss = ExperimentParams.target_loss\n",
    "        self.initial_loss = self.get_loss(self.seed).numpy()\n",
    "        self.pool = SamplePool(x = np.repeat(self.seed[None, ...], SamplePool.pool_size, 0))\n",
    "        \n",
    "    def generate_seed_state(self):\n",
    "        h, w = self.target.shape[:2]\n",
    "        x0 = np.ones([h, w, CellularAutomata.channel_n], np.float32) * 0.5\n",
    "        \n",
    "        # Randomly scramble the seed image just a little.\n",
    "        #for i in range(h * w * CellularAutomata.channel_n // ExperimentParams.seed_mutation_odds):\n",
    "        #    x0[random.randrange(h), random.randrange(w), random.randrange(CellularAutomata.channel_n)] = random.random()\n",
    "        \n",
    "        return x0\n",
    "        \n",
    "    def get_loss(self, x):\n",
    "        return tf.reduce_mean(tf.square(to_rgb(x) - self.target), [-2, -3, -1])\n",
    "    \n",
    "    def get_sum(self, x):\n",
    "        return tf.reduce_sum(x)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, x):\n",
    "        iter_n = ExperimentParams.lifetime\n",
    "        iter_n = tf.random.uniform([], 10, ExperimentParams.lifetime, tf.int32)\n",
    "        with tf.GradientTape() as g:\n",
    "            for i in tf.range(iter_n):\n",
    "                x = self.ca(x)\n",
    "            loss = tf.reduce_mean(self.get_loss(x))\n",
    "            \n",
    "        grads = g.gradient(loss, self.ca.weights)\n",
    "        grads = [g / (tf.norm(g) + 1.0e-8) for g in grads]\n",
    "        self.trainer.apply_gradients(zip(grads, self.ca.weights))\n",
    "        return x, loss\n",
    "        \n",
    "    def show_status(self):\n",
    "        for i in range(5):\n",
    "            # Run the CA for its lifetime with the current weights.\n",
    "            xs = []\n",
    "            x = self.generate_seed_state()[None, ...]\n",
    "            for i in range(random.randrange(10, ExperimentParams.lifetime)):\n",
    "                x = self.ca(x)\n",
    "                xs.append(x)\n",
    "                \n",
    "            result_image = np.hstack([np.hstack(to_rgb(x).numpy()) for x in xs])\n",
    "            imshow(zoom(result_image, 3))\n",
    "        \n",
    "        print(\"\\n step: %d, log10(loss): %.3f\" % (\n",
    "            len(self.loss_hist), np.log10(self.loss_hist[-1])), end='')\n",
    "    \n",
    "    def run(self, num_steps, batch_size = 1):\n",
    "        initial = result = loss = None\n",
    "        assert num_steps > 0\n",
    "        for i in range(num_steps):\n",
    "            initial = np.repeat(self.seed[None, ...], batch_size, 0)\n",
    "            x, loss = self.train_step(initial)\n",
    "            self.loss_hist.append(loss.numpy())\n",
    "            if np.log10(loss) <= self.target_loss:\n",
    "                break\n",
    "\n",
    "def test_training():\n",
    "    print(vars(ExperimentParams))\n",
    "    training = TrainingConfig()\n",
    "    imshow(zoom(training.target, 5))\n",
    "\n",
    "    def run():\n",
    "        while not training.loss_hist or min(np.log10(training.loss_hist)) > ExperimentParams.target_loss:\n",
    "            training.run(num_steps = 25)\n",
    "            training.show_status()\n",
    "    import timeit\n",
    "    print(\"\\n Time (s): %.3f\" % timeit.timeit(run, number=1))\n",
    "    print(\"\\n Best loss: %.3f\" % min(np.log10(training.loss_hist)))\n",
    "    print(\"\\n Num steps: %d\" % len(training.loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated testing on different hyperparameters.\n",
    "timeout_steps = 1000\n",
    "default_params = ExperimentParams\n",
    "\n",
    "def run_experiment(params: dict):\n",
    "    global ExperimentParams\n",
    "    ExperimentParams = default_params\n",
    "    for param, value in params.items():\n",
    "        setattr(ExperimentParams, param, value)\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    while not training.loss_hist or min(np.log10(training.loss_hist)) > ExperimentParams.target_loss and len(training.loss_hist) < timeout_steps:\n",
    "        training.run(num_steps = 10)\n",
    "        print(\".\", end=\"\")\n",
    "    print(f\"\\n   steps: {len(training.loss_hist)}\")\n",
    "\n",
    "    #channel_count = 32\n",
    "    #filter_size = 32 # number of neurons in middle layer\n",
    "    #pool_size = 1\n",
    "    #learning_rate = 3.0e-3\n",
    "    #lifetime = 50\n",
    "    \n",
    "experiments = [\n",
    "    {\"channel_count\": 2}, {\"channel_count\": 256},\n",
    "    {\"filter_size\": 2}, {\"filter_size\": 256},\n",
    "    {\"pool_size\": 1}, {\"pool_size\": 32},\n",
    "    {\"learning_rate\": 1.0e-3}, {\"learning_rate\": 4.0e-3},\n",
    "    {\"lifetime\": 15}, {\"lifetime\": 200},\n",
    "]\n",
    "\n",
    "for i in range(len(experiments)):\n",
    "    print(f\"Experiment {i} -- \", experiments[i], \" -- \", end=\"\")\n",
    "    run_experiment(experiments[i])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class ExperimentParams:\n",
    "\n",
    "    # ----------------------------\n",
    "    # Parameters to tune:\n",
    "    # ----------------------------\n",
    "    channel_count = 32\n",
    "    filter_size = 32 # number of neurons in middle layer\n",
    "    pool_size = 1\n",
    "    learning_rate = 3.0e-3\n",
    "    lifetime = 50\n",
    "\n",
    "    # ----------------------------\n",
    "    # Challenge parameters:\n",
    "    # ----------------------------\n",
    "    image_data = \"checkerboard8.png\"\n",
    "    target_loss = -2.2\n",
    "    noise = 0.1\n",
    "    conserve_mass = False\n",
    "    clamp_values = True\n",
    "    \n",
    "    #seed_mutation_odds = 100000000\n",
    "\n",
    "Experiment 0 --  {'channel_count': 2}  -- ...........\n",
    "   steps: 108\n",
    "Experiment 1 --  {'channel_count': 256}  -- .........\n",
    "   steps: 85\n",
    "Experiment 2 --  {'filter_size': 2}  -- ....................................................................................................\n",
    "   steps: 1000\n",
    "Experiment 3 --  {'filter_size': 256}  -- ........\n",
    "   steps: 77\n",
    "Experiment 4 --  {'pool_size': 1}  -- ..........\n",
    "   steps: 96\n",
    "Experiment 5 --  {'pool_size': 32}  -- ............\n",
    "   steps: 119\n",
    "Experiment 6 --  {'learning_rate': 0.001}  -- .........\n",
    "   steps: 84\n",
    "Experiment 7 --  {'learning_rate': 0.004}  -- ...............................\n",
    "   steps: 308\n",
    "Experiment 8 --  {'lifetime': 15}  -- .......\n",
    "   steps: 61\n",
    "Experiment 9 --  {'lifetime': 200}  -- ....................................................................................................\n",
    "   steps: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: testing for target loss rather than number of steps\n",
    "\n",
    "# Tests on checkerboard 8x8:\n",
    "# (channel_count, filter_size) => (# steps, time)\n",
    "data1 = {\n",
    "    (4, 32): (1130, 22.3),\n",
    "    (128, 32): (112, 5.868),\n",
    "    (256, 32): (88, 7.464),\n",
    "    \n",
    "    (4, 4): None, # timed-out at 15,000 steps\n",
    "    (4, 8): None, # timed-out at 10,000 steps\n",
    "    (4, 16): (3085, 56.941),\n",
    "    \n",
    "    (6, 6): (6458, 120.829),\n",
    "    (8, 8): (2025, 38.033),\n",
    "    (8, 16): (584, 12.230),\n",
    "    \n",
    "    (8, 32): (489, 10.062),\n",
    "    (8, 64): (233, 6.113),\n",
    "    (8, 128): (100, 4.295),\n",
    "    \n",
    "    (32, 32): (237, 7.401),\n",
    "    (64, 64): (83, 3.878),\n",
    "    (128, 128): (95, 7.739),\n",
    "}\n",
    "\n",
    "# Tests on square 8x8:\n",
    "# (channel_count, filter_size) => (# steps, time)\n",
    "data1 = {\n",
    "    (4, 4): None, # couldn't learn at all,\n",
    "    (64, 64): None, # couldn't learn at all\n",
    "}\n",
    "\n",
    "# Tests on guided square 8x8:\n",
    "# (channel_count, filter_size) => (# steps, time)\n",
    "data1 = {\n",
    "    (4, 4): None, #timed-out at 8,300 steps\n",
    "    (32, 32): (74, 3.052),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: testing on various images and trying to improve time to train\n",
    "\n",
    "# Hypothesis: increasing fire rate will improve loss and not affect time\n",
    "data3 = {\n",
    "    0.1: (-0.417, 90),\n",
    "    1.0: (-2.415, 87),\n",
    "}\n",
    "# Removing the update mask calculations entirely reduced time to 84 seconds\n",
    "\n",
    "# Hypothesis: increasing learning rate will first improve loss and then worsen it and not affect time\n",
    "# pool_size => (min_loss, time)\n",
    "data2 = {\n",
    "    0.1e-3: (-0.344, 51),\n",
    "    2.0e-3: (-1.737, 51),\n",
    "    3.0e-3: (-1.742, 52),\n",
    "    5.0e-3: (-1.751, 52),\n",
    "    10.0e-3: (-1.751, 52),\n",
    "    20.0e-3: (-1.639, 53),\n",
    "}\n",
    "\n",
    "# Hypothesis: increasing pool size will improve loss but increase time\n",
    "# pool_size => (min_loss, time)\n",
    "data2 = {\n",
    "    8: (-1.737, 52),\n",
    "    32: (-1.724, 53),\n",
    "    128: (-1.733, 52),\n",
    "    1024: (-1.736, 51),\n",
    "}\n",
    "\n",
    "# Hypothesis: increasing lifetime will improve loss but increase time\n",
    "# lifetime => (min_loss, time)\n",
    "data1 = {\n",
    "    10: (-0.457, 26),\n",
    "    30: (-1.731, 51),\n",
    "    50: (-1.757, 86),\n",
    "    100: (-1.761, 204),\n",
    "    200: (-1.765, 362),\n",
    "}\n",
    "\n",
    "# 1 -> 2 : 10 times\n",
    "# 1 -> 1.1 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: testing on Lenna 32x32 and examining channel counts\n",
    "\n",
    "# Hypothesis: increasing channel count will reduce best loss\n",
    "\n",
    "# Filter size => (Best loss, Time)\n",
    "filter_sizes = {\n",
    "    '4': (-1.674, 134),\n",
    "    '64': (-1.754, 225),\n",
    "}\n",
    "\n",
    "# Channel count => Best Loss\n",
    "channel_counts = {\n",
    "    '4': -1.688,\n",
    "    '5': -1.735,\n",
    "    '6': -1.705,\n",
    "    '8': -1.733,\n",
    "    '10': -1.727,\n",
    "    '12': -1.751,\n",
    "    '14': -1.732,\n",
    "    '16': -1.719,\n",
    "    '32': -1.742,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meeting Notes:\n",
    "\n",
    "- Seed the random numbers the same way, should behave exactly the same\n",
    "    - Look into that kind of thing in Tensorflow\n",
    "- Record the seed when an experiment starts\n",
    "- The parameters are not separable\n",
    "    - Separable parameters are independent, when you fix one it doesn't affect the other\n",
    "    - Try each parameter change with a change in each other parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bit7f79f381b2994605bf95c96e7d268b80"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
